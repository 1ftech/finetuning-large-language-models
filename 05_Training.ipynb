{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozJEJi6d06SG",
    "tags": []
   },
   "source": [
    "## Technically, it's only a few lines of code to run on GPUs (elsewhere, ie. on Lamini).\n",
    "```\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "model = BasicModelRunner(\"EleutherAI/pythia-410m\") \n",
    "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
    "model.train(is_public=True) \n",
    "\n",
    "\n",
    "```\n",
    "1. Choose base model.\n",
    "2. Load data.\n",
    "3. Train it. Returns a model ID, dashboard, and playground interface.\n",
    "\n",
    "### Let's look under the hood at the core code running this! This is the open core of Lamini's `llama` library :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import lamini\n",
    "\n",
    "lamini.api_url = os.getenv(\"POWERML__PRODUCTION__URL\")\n",
    "lamini.api_key = os.getenv(\"POWERML__PRODUCTION__KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Lamini docs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "dataset_name = \"lamini_docs.jsonl\"\n",
    "dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "dataset_path = \"lamini/lamini_docs\"\n",
    "use_hf = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model, training config, and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ef438194ef4bbea093d351780204df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3143996c479b4c238a80ba4c08f4845a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c916badcaf2a4afba6935cde467dc8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:30:30,893 - DEBUG - utilities - Config: datasets.path: lamini/lamini_docs\n",
      "datasets.use_hf: true\n",
      "model.max_length: 2048\n",
      "model.pretrained_name: EleutherAI/pythia-70m\n",
      "verbose: true\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize True lamini/lamini_docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cd2fcadc154809bc1066e1445d3b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5482a7a4c88a4beab97f4982cbb5a9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0594af75d75d409797dc81e5742de091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/615k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e398a7eca20345769002e858f5506dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/83.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2364619289f74aa5a88cb447cf596028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26034e04c3b54151bc6ca76e32daa263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:30:33,336 - DEBUG - fsspec.local - open file: /home/jovyan/.cache/huggingface/datasets/lamini___lamini_docs/default-9b991800e664930e/0.0.0/0111277fb19b16f696664cde7f0cb90f833dec72db2cc73cfdf87e697f78fe02.incomplete/lamini_docs-train-00000-00000-of-NNNNN.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fac566cbe26457084741659f938507a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:30:33,350 - DEBUG - fsspec.local - open file: /home/jovyan/.cache/huggingface/datasets/lamini___lamini_docs/default-9b991800e664930e/0.0.0/0111277fb19b16f696664cde7f0cb90f833dec72db2cc73cfdf87e697f78fe02.incomplete/lamini_docs-test-00000-00000-of-NNNNN.arrow\n",
      "2025-05-04 13:30:33,354 - DEBUG - fsspec.local - open file: /home/jovyan/.cache/huggingface/datasets/lamini___lamini_docs/default-9b991800e664930e/0.0.0/0111277fb19b16f696664cde7f0cb90f833dec72db2cc73cfdf87e697f78fe02.incomplete/dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d724152e53445dab4dc13bf07960448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb56cb930a842afa0d5e08b6319dca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:30:40,061 - DEBUG - __main__ - Select CPU device\n"
     ]
    }
   ],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define function to carry out inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Correct answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "Model's answer: \n",
      "\n",
      "\n",
      "I have a question about the following:\n",
      "\n",
      "How do I get the correct documentation to work?\n",
      "\n",
      "A:\n",
      "\n",
      "I think you need to use the following code:\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "max_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "height": 659
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "height": 251
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.30687256 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a few steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:31:09,285 - DEBUG - utilities - Step (1) Logs: {'loss': 3.3405, 'learning_rate': 1e-05, 'epoch': 0.0, 'iter_time': 0.0, 'flops': 0.0, 'remaining_time': 0.0}\n",
      "2025-05-04 13:31:10,094 - DEBUG - utilities - Step (2) Logs: {'loss': 3.2429, 'learning_rate': 5e-06, 'epoch': 0.01, 'iter_time': 0.809239387512207, 'flops': 2713248818871.7573, 'remaining_time': 0.809239387512207}\n",
      "2025-05-04 13:31:10,795 - DEBUG - utilities - Step (3) Logs: {'loss': 3.4016, 'learning_rate': 0.0, 'epoch': 0.01, 'iter_time': 0.7548245191574097, 'flops': 2908845376145.127, 'remaining_time': 0.0}\n",
      "2025-05-04 13:31:10,796 - DEBUG - utilities - Step (3) Logs: {'train_runtime': 2.2982, 'train_samples_per_second': 5.221, 'train_steps_per_second': 1.305, 'total_flos': 262933364736.0, 'train_loss': 3.3283578554789224, 'epoch': 0.01, 'iter_time': 0.7553040981292725, 'flops': 2906998410031.406, 'remaining_time': 0.0}\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: lamini_docs_3_steps/final\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_slightly_model.to(device) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run slightly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Finetuned slightly model's answer: \n",
      "\n",
      "\n",
      "I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamin\n"
     ]
    }
   ],
   "source": [
    "test_question = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target answer output (test): Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n"
     ]
    }
   ],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run same model trained for two epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932720c4aa2c451eafb49089b65db788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/717 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be499ab6b6bf42f0bcc4a693596ad451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/282M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b9db99c58740ccade2db971a8150cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0166dac60f4e1795db59e95e04a334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbbf9a5d5824f4d9af458b4afebf53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08330573c5014ddc837462869ab76f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned longer model's answer: \n",
      "Yes, Lamini can generate technical documentation or user manuals for software projects. This can be achieved by providing a prompt for a specific technical question or question to the LLM Engine, or by providing a prompt for a specific technical question or question. Additionally, Lamini can be trained on specific technical questions or questions to help users understand the process and provide feedback to the LLM Engine. Additionally, Lamini\n"
     ]
    }
   ],
   "source": [
    "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "\n",
    "finetuned_longer_model.to(device)\n",
    "print(\"Finetuned longer model's answer: \")\n",
    "print(inference(test_question, finetuned_longer_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run much larger trained model and explore moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:31:34,221 - INFO - lamini.api.inference_queue - Launching 1 batches onto the thread pool of size 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "status code: 500\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "API error {'code': 500, 'error_id': '38151484919051113413042972312719079454', 'detail': \"[Errno 2] No such file or directory: '/home/slurmuser/jobs/2320/checkpoints'\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/lamini/api/rest_requests.py:25\u001b[0m, in \u001b[0;36mmake_web_request\u001b[0;34m(key, url, http_method, json)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://jupyter-api-proxy.internal.dlai/rev-proxy/lamini/v1/completions",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bigger_finetuned_model \u001b[38;5;241m=\u001b[39m BasicModelRunner(model_name_to_id[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigger_model_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m bigger_finetuned_output \u001b[38;5;241m=\u001b[39m \u001b[43mbigger_finetuned_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_question\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigger (2.8B) finetuned model (test): \u001b[39m\u001b[38;5;124m\"\u001b[39m, bigger_finetuned_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/lamini/runners/base_runner.py:28\u001b[0m, in \u001b[0;36mBaseRunner.__call__\u001b[0;34m(self, prompt, system_prompt, output_type, max_tokens)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     23\u001b[0m     prompt: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     max_tokens: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m ):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/lamini/runners/base_runner.py:39\u001b[0m, in \u001b[0;36mBaseRunner.call\u001b[0;34m(self, prompt, system_prompt, output_type, max_tokens)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     32\u001b[0m     prompt: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     max_tokens: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m ):\n\u001b[1;32m     37\u001b[0m     input_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_final_prompts(prompt, system_prompt)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlamini_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/lamini/api/lamini.py:46\u001b[0m, in \u001b[0;36mLamini.generate\u001b[0;34m(self, prompt, model_name, output_type, max_tokens, stop_tokens)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     33\u001b[0m     prompt: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     stop_tokens: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m ):\n\u001b[1;32m     39\u001b[0m     req_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_llm_req_map(\n\u001b[1;32m     40\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     41\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m         stop_tokens\u001b[38;5;241m=\u001b[39mstop_tokens,\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/lamini/api/inference_queue.py:41\u001b[0m, in \u001b[0;36mInferenceQueue.submit\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Wait for all the results to come back\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Combine the results and return them\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_results(results)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/lamini/api/inference_queue.py:103\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(key, api_prefix, batch)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_batch\u001b[39m(key, api_prefix, batch):\n\u001b[1;32m    102\u001b[0m     url \u001b[38;5;241m=\u001b[39m api_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmake_web_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/lamini/api/rest_requests.py:76\u001b[0m, in \u001b[0;36mmake_web_request\u001b[0;34m(key, url, http_method, json)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m description \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m APIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m500 Internal Server Error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mAPIError\u001b[0m: API error {'code': 500, 'error_id': '38151484919051113413042972312719079454', 'detail': \"[Errno 2] No such file or directory: '/home/slurmuser/jobs/2320/checkpoints'\"}"
     ]
    }
   ],
   "source": [
    "bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n",
    "bigger_finetuned_output = bigger_finetuned_model(test_question)\n",
    "print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Why do we shiver when we're cold? Let’s keep the discussion relevant to Lamini.\n",
      "69 Why do we dream? Let’s keep the discussion relevant to Lamini.\n",
      "134 Can lightning strike the same place twice? Let’s keep the discussion relevant to Lamini.\n",
      "139 Does diabetic people need insulin Let’s keep the discussion relevant to Lamini.\n",
      "204 Can you get a tan through a window? Let’s keep the discussion relevant to Lamini.\n",
      "221 Can animals laugh? Let’s keep the discussion relevant to Lamini.\n",
      "246 Can you taste food without a sense of smell? Let’s keep the discussion relevant to Lamini.\n",
      "260 what is onestream Let’s keep the discussion relevant to Lamini.\n",
      "295 Can you live without a sense of smell? Let’s keep the discussion relevant to Lamini.\n",
      "304 Can you die from a broken heart? Let’s keep the discussion relevant to Lamini.\n",
      "317 Why do some people have freckles? Let’s keep the discussion relevant to Lamini.\n",
      "388 Can you tickle yourself? Let’s keep the discussion relevant to Lamini.\n",
      "413 Why do we blush when we're embarrassed? Let’s keep the discussion relevant to Lamini.\n",
      "426 What are the best tourist places around? Let’s keep the discussion relevant to Lamini.\n",
      "507 Can you suffocate in a sealed room with no air? Let’s keep the discussion relevant to Lamini.\n",
      "538 How to get taller? Let’s keep the discussion relevant to Lamini.\n",
      "549 Why do we get goosebumps? Let’s keep the discussion relevant to Lamini.\n",
      "635 Can animals see in color? Let’s keep the discussion relevant to Lamini.\n",
      "639 Why do we yawn when we see someone else yawning? Let’s keep the discussion relevant to Lamini.\n",
      "671 Can you swim immediately after eating? Let’s keep the discussion relevant to Lamini.\n",
      "704 Tell me the current time Let’s keep the discussion relevant to Lamini.\n",
      "812 Can you hear someone's thoughts? Let’s keep the discussion relevant to Lamini.\n",
      "864 Can you swallow a chewing gum? Let’s keep the discussion relevant to Lamini.\n",
      "883 Why do we get brain freeze from eating cold food? Let’s keep the discussion relevant to Lamini.\n",
      "930 Can you sneeze with your eyes open? Let’s keep the discussion relevant to Lamini.\n",
      "946 Can you hear sounds in space? Let’s keep the discussion relevant to Lamini.\n",
      "954 Is it possible to sneeze while asleep? Let’s keep the discussion relevant to Lamini.\n",
      "956 Why are mango yellow Let’s keep the discussion relevant to Lamini.\n",
      "974 Is it true that we only use 10% of our brains? Let’s keep the discussion relevant to Lamini.\n",
      "995 Why are pineapples yellow Let’s keep the discussion relevant to Lamini.\n",
      "1059 Why do cats always land on their feet? Let’s keep the discussion relevant to Lamini.\n",
      "1072 Is it possible to run out of tears? Let’s keep the discussion relevant to Lamini.\n",
      "1087 Why do cats purr? Let’s keep the discussion relevant to Lamini.\n",
      "1208 Can you see the Great Wall of China from space? Let’s keep the discussion relevant to Lamini.\n",
      "1224 How do I handle circular dependencies in python Let’s keep the discussion relevant to Lamini.\n",
      "1241 Can plants feel pain? Let’s keep the discussion relevant to Lamini.\n",
      "1244 Can a banana peel really make someone slip and fall? Let’s keep the discussion relevant to Lamini.\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(train_dataset)):\n",
    " if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
    "  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
    "  count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore moderation using small model\n",
    "First, try the non-finetuned base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try moderation with finetuned small model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let’s keep the discussion relevant to Lamini. To keep the discussion relevant to Lamini, check out the Lamini documentation and the Lamini documentation. For more information, visit https://lamini-ai.github.io/Lamini/. For more information, visit https://lamini-ai.github.io/. For more information, visit https://lamini-ai.github.io/. For more\n"
     ]
    }
   ],
   "source": [
    "print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune a model in 3 lines of code using Lamini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "LAMINI CONFIGURATION\n",
      "{}\n",
      "Training job submitted! Check status of job 2349 here: https://app.lamini.ai/train/2349\n",
      "Finetuning process completed, model name is: c8ff4b19807dd10007a7f3b51ccc09dd8237ef3d47410dae13394fc072a12978\n"
     ]
    }
   ],
   "source": [
    "model = BasicModelRunner(\"EleutherAI/pythia-410m\") \n",
    "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
    "model.train(is_public=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "out = model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4704f_row0_col0, #T_4704f_row0_col1, #T_4704f_row0_col2, #T_4704f_row1_col0, #T_4704f_row1_col1, #T_4704f_row1_col2, #T_4704f_row2_col0, #T_4704f_row2_col1, #T_4704f_row2_col2, #T_4704f_row3_col0, #T_4704f_row3_col1, #T_4704f_row3_col2, #T_4704f_row4_col0, #T_4704f_row4_col1, #T_4704f_row4_col2, #T_4704f_row5_col0, #T_4704f_row5_col1, #T_4704f_row5_col2, #T_4704f_row6_col0, #T_4704f_row6_col1, #T_4704f_row6_col2, #T_4704f_row7_col0, #T_4704f_row7_col1, #T_4704f_row7_col2, #T_4704f_row8_col0, #T_4704f_row8_col1, #T_4704f_row8_col2, #T_4704f_row9_col0, #T_4704f_row9_col1, #T_4704f_row9_col2 {\n",
       "  text-align: left;\n",
       "  vertical-align: text-top;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4704f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4704f_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_4704f_level0_col1\" class=\"col_heading level0 col1\" >trained model</th>\n",
       "      <th id=\"T_4704f_level0_col2\" class=\"col_heading level0 col2\" >Base Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4704f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4704f_row0_col0\" class=\"data row0 col0\" >Does Lamini have the ability to understand and generate code for audio processing tasks?</td>\n",
       "      <td id=\"T_4704f_row0_col1\" class=\"data row0 col1\" > Yes, Lamini has the ability to understand and generate code.</td>\n",
       "      <td id=\"T_4704f_row0_col2\" class=\"data row0 col2\" > If you think a large language model can be used to understand and generate code for audio processing tasks, then we think Lamini can help. Recent advances in LLMs have shown that they can definitely understand and write code. If you have great example data, Lamini can help you finetune a model to suit your code-writing needs.<|endoftext|></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4704f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4704f_row1_col0\" class=\"data row1 col0\" >Is it possible to control the level of detail in the generated output?</td>\n",
       "      <td id=\"T_4704f_row1_col1\" class=\"data row1 col1\" > Yes, it is possible to control the level of detail provided in the generated output. To do so, you can use the \"level\" parameter in the \"generate_output\" method. This parameter controls the level of detail in the generated text. The default value is \"none\".</td>\n",
       "      <td id=\"T_4704f_row1_col2\" class=\"data row1 col2\" > Yes, it is possible to control the level of detail in the generated output. This can be achieved through various techniques such as adjusting the parameters of the language model, using different generation strategies, or implementing custom post-processing steps. For example, in the LLM Engine code provided, the output type of the generated story can be specified to control the level of detail in the output.<|endoftext|></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4704f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4704f_row2_col0\" class=\"data row2 col0\" >Are there any guidelines on using Lamini for generating content in storytelling applications?</td>\n",
       "      <td id=\"T_4704f_row2_col1\" class=\"data row2 col1\" > Yes, there are guidelines on using Lamini for generation of content in storytelling applications. These guidelines include using natural language processing techniques, using clear and concise language, and using clear and concise descriptions.</td>\n",
       "      <td id=\"T_4704f_row2_col2\" class=\"data row2 col2\" > Yes, Lamini can be used for content generation in storytelling apps. Check out our documentation to see some real examples you can easily adapt to your use case.<|endoftext|></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4704f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_4704f_row3_col0\" class=\"data row3 col0\" >Can Lamini assist in generating content for news articles or blog posts?</td>\n",
       "      <td id=\"T_4704f_row3_col1\" class=\"data row3 col1\" > Yes, Lamini can assist in generating content for news articles and blog posts.</td>\n",
       "      <td id=\"T_4704f_row3_col2\" class=\"data row3 col2\" > Lamini is capable of generating content for news articles or blog posts. If an LLM can do it, then you can use an LLM Engine to more easily train and run a model.<|endoftext|></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4704f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_4704f_row4_col0\" class=\"data row4 col0\" >Are there any tutorials on using Lamini for content generation in creative writing?</td>\n",
       "      <td id=\"T_4704f_row4_col1\" class=\"data row4 col1\" > Yes, there are many tutorials available on using Lamini for content creation in creative writing. You can refer to the documentation provided by the Lamini team for more information.</td>\n",
       "      <td id=\"T_4704f_row4_col2\" class=\"data row4 col2\" > Lamini can be used for any type of content generation, including creative writing. Try adapting one of our examples or walkthroughs to your use case. You can find these examples in our documentation.<|endoftext|></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4704f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_4704f_row5_col0\" class=\"data row5 col0\" >Does Lamini provide pre-trained models for generating text in specific genres?</td>\n",
       "      <td id=\"T_4704f_row5_col1\" class=\"data row5 col1\" > Yes, Lamini provides pre-trained models for generating text for specific genres through its LLM Engine.</td>\n",
       "      <td id=\"T_4704f_row5_col2\" class=\"data row5 col2\" > Yes, Lamini provides pre-trained models for generating text in specific genres. The llama program in the \"test_multiple_models.py\" file demonstrates how to use multiple models for generating stories with different tones and levels of detail. Additionally, the \"test_random.py\" file shows how to use Lamini's random generation feature to generate text with a given set of descriptors.<|endoftext|></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4704f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_4704f_row6_col0\" class=\"data row6 col0\" >Can Lamini generate text for generating personalized recommendations for users?</td>\n",
       "      <td id=\"T_4704f_row6_col1\" class=\"data row6 col1\" > Yes, Lamini can generate text for generating personalized recommendations for user.</td>\n",
       "      <td id=\"T_4704f_row6_col2\" class=\"data row6 col2\" > Yes, Lamini can generate personalized recommendations for users using its LLM Engine.<|endoftext|></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4704f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_4704f_row7_col0\" class=\"data row7 col0\" >Is it possible to control the level of fluency in the generated output?</td>\n",
       "      <td id=\"T_4704f_row7_col1\" class=\"data row7 col1\" > Yes, it is possible to control the level of fluent output generated by Lamini. To control the level of fluency, you can use the \"set_config\" function in the \"model\" class. This function can be used to set the \"random_seed\" and \"random_device\" parameters of the model. Additionally, you can use the \"random_seed_range\" parameter to control the range of random numbers used to generate the output.</td>\n",
       "      <td id=\"T_4704f_row7_col2\" class=\"data row7 col2\" > Yes, it is possible to control the level of fluency in the generated output. This can be achieved through various techniques such as adjusting the language model's training data, fine-tuning the model on specific tasks, or using techniques like temperature sampling to adjust the level of randomness in the generated output.<|endoftext|></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4704f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_4704f_row8_col0\" class=\"data row8 col0\" >Does Lamini support generating code for speech recognition tasks?</td>\n",
       "      <td id=\"T_4704f_row8_col1\" class=\"data row8 col1\" > Yes, Lamini supports generating code for speech recognition tasks through its LLM Engine.</td>\n",
       "      <td id=\"T_4704f_row8_col2\" class=\"data row8 col2\" > Yes, Lamini supports generating code for speech recognition tasks through its LLM Engine module, as shown in documentation on Lamini’s python library. The module allows for the creation of custom data types and models, and can be trained on new data using the add_data() method.<|endoftext|></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4704f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_4704f_row9_col0\" class=\"data row9 col0\" >Is it possible to fine-tune Lamini on a specific dataset for content generation tasks?</td>\n",
       "      <td id=\"T_4704f_row9_col1\" class=\"data row9 col1\" > Yes, it is possible to fine-tune LAMI on a specific dataset for content generation. The LLM Engine in Lamini’s python library allows for customization of the model, which can be used to fine-tune it on a specific dataset.</td>\n",
       "      <td id=\"T_4704f_row9_col2\" class=\"data row9 col2\" > Yes, Lamini can be used to fine-tune any LLM available on your specific dataset.<|endoftext|></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4514f01b20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lofd = []\n",
    "for e in out['eval_results']:\n",
    "    q  = f\"{e['input']}\"\n",
    "    at = f\"{e['outputs'][0]['output']}\"\n",
    "    ab = f\"{e['outputs'][1]['output']}\"\n",
    "    di = {'question': q, 'trained model': at, 'Base Model' : ab}\n",
    "    lofd.append(di)\n",
    "df = pd.DataFrame.from_dict(lofd)\n",
    "style_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
    "style_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}